# Spark Default Configuration for Testing
# Aligned with Pedregal production defaults

# Basic settings
spark.master                                    local[*]
spark.driver.memory                             2g
spark.executor.memory                           2g
spark.sql.shuffle.partitions                    10

# Spark Connect settings
spark.connect.grpc.binding.port                 15002
spark.connect.grpc.arrow.maxBatchSize           10485760

# Iceberg/Unity Catalog configuration
spark.sql.extensions                            org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog                 org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type            hive

# S3 configuration (for MinIO/LocalStack in tests)
spark.hadoop.fs.s3a.endpoint                    http://localhost:9000
spark.hadoop.fs.s3a.access.key                  minioadmin
spark.hadoop.fs.s3a.secret.key                  minioadmin
spark.hadoop.fs.s3a.path.style.access           true
spark.hadoop.fs.s3a.impl                        org.apache.hadoop.fs.s3a.S3AFileSystem

# Logging
spark.eventLog.enabled                          true
spark.eventLog.dir                              /tmp/spark-events
spark.history.fs.logDirectory                   /tmp/spark-events

# OpenTelemetry (for observability)
spark.otel.service.name                         spark-test-job
spark.plugins                                   org.apache.spark.metrics.DropwizardPlugin

# Performance tuning for tests
spark.sql.adaptive.enabled                      true
spark.sql.adaptive.coalescePartitions.enabled   true
spark.dynamicAllocation.enabled                 false

# Serialization
spark.serializer                                org.apache.spark.serializer.KryoSerializer
