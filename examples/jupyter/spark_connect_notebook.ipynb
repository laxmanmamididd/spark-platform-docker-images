{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Connect Interactive Notebook\n",
    "\n",
    "This notebook demonstrates how to use Spark Connect for interactive data exploration and feature engineering at DoorDash.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install pyspark[connect] pandas\n",
    "```\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **No local Spark installation** - Just pip install the thin client\n",
    "- **Unity Catalog access** - Pre-configured catalog access\n",
    "- **Team isolation** - Each team gets their own cluster\n",
    "- **Environment separation** - dev/staging/prod clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using our helper library\n",
    "import sys\n",
    "sys.path.insert(0, '../../spark-connect-client/python')\n",
    "from spark_connect_client import SparkConnectClient\n",
    "\n",
    "spark = SparkConnectClient.create_session(\n",
    "    team=\"feature-engineering\",\n",
    "    environment=\"dev\",\n",
    "    region=\"us-west-2\",\n",
    "    app_name=\"interactive-exploration\"\n",
    ")\n",
    "\n",
    "print(f\"Connected to Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Direct connection (if you know the endpoint)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .remote(\"sc://feature-eng-dev-uswest2.doordash.team:15002\") \\\n",
    "#     .appName(\"interactive-exploration\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available catalogs\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List databases in pedregal catalog\n",
    "spark.sql(\"SHOW DATABASES IN pedregal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in the raw database\n",
    "spark.sql(\"SHOW TABLES IN pedregal.raw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe a table\n",
    "spark.sql(\"DESCRIBE TABLE pedregal.raw.events\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data from events table\n",
    "events_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM pedregal.raw.events\n",
    "    WHERE ds = '2024-01-01'\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "events_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count events by type\n",
    "event_counts = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        event_type,\n",
    "        COUNT(*) as count,\n",
    "        COUNT(DISTINCT user_id) as unique_users\n",
    "    FROM pedregal.raw.events\n",
    "    WHERE ds = '2024-01-01'\n",
    "    GROUP BY event_type\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "event_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "event_counts_pd = event_counts.toPandas()\n",
    "event_counts_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user-level features\n",
    "user_features = spark.sql(\"\"\"\n",
    "    WITH user_events AS (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            event_type,\n",
    "            event_timestamp,\n",
    "            properties\n",
    "        FROM pedregal.raw.events\n",
    "        WHERE ds >= date_sub(current_date(), 7)\n",
    "          AND user_id IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        user_id,\n",
    "        \n",
    "        -- Activity features\n",
    "        COUNT(*) as total_events_7d,\n",
    "        COUNT(DISTINCT DATE(event_timestamp)) as active_days_7d,\n",
    "        \n",
    "        -- Event type features\n",
    "        SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) as views_7d,\n",
    "        SUM(CASE WHEN event_type = 'click' THEN 1 ELSE 0 END) as clicks_7d,\n",
    "        SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchases_7d,\n",
    "        \n",
    "        -- Derived features\n",
    "        CASE\n",
    "            WHEN SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END) > 0\n",
    "            THEN ROUND(\n",
    "                SUM(CASE WHEN event_type = 'click' THEN 1 ELSE 0 END) * 1.0 /\n",
    "                SUM(CASE WHEN event_type = 'view' THEN 1 ELSE 0 END), 4\n",
    "            )\n",
    "            ELSE 0\n",
    "        END as ctr_7d\n",
    "        \n",
    "    FROM user_events\n",
    "    GROUP BY user_id\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Computed features for {user_features.count()} users\")\n",
    "user_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "user_features.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Pandas for model training\n",
    "features_pd = user_features.toPandas()\n",
    "print(f\"Exported {len(features_pd)} rows to Pandas\")\n",
    "features_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write Results Back to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date partition\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "user_features_with_date = user_features.withColumn(\"ds\", current_date())\n",
    "user_features_with_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to feature store (create or append)\n",
    "# Note: In dev environment, we use a dev-specific table\n",
    "\n",
    "user_features_with_date.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .saveAsTable(\"pedregal.feature_store_dev.user_features_exploration\")\n",
    "\n",
    "print(\"Features written to Unity Catalog!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Read events using DataFrame API\n",
    "events = spark.table(\"pedregal.raw.events\") \\\n",
    "    .filter(F.col(\"ds\") == \"2024-01-01\") \\\n",
    "    .filter(F.col(\"user_id\").isNotNull())\n",
    "\n",
    "# Compute running totals per user\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"event_timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "events_with_running = events \\\n",
    "    .withColumn(\"event_number\", F.row_number().over(Window.partitionBy(\"user_id\").orderBy(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"running_events\", F.count(\"*\").over(window_spec))\n",
    "\n",
    "events_with_running.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tips for Interactive Development\n",
    "\n",
    "1. **Use `.cache()` wisely** - Cache DataFrames you'll reuse multiple times\n",
    "2. **Use `.limit()` for exploration** - Don't pull full tables unnecessarily\n",
    "3. **Use `.explain()` to understand query plans** - Optimize before running expensive queries\n",
    "4. **Use dev environment for exploration** - Switch to staging/prod for validation\n",
    "5. **Stop your session when done** - Free up cluster resources for others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
